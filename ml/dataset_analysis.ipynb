{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c26b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Set style for better visualizations\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Load the dataset\n",
    "dataset_path = Path('future_skills_dataset.csv')\n",
    "if not dataset_path.exists():\n",
    "    print(\"‚ö†Ô∏è Dataset not found. Please run: python ../manage.py export_future_skills_dataset\")\n",
    "else:\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    print(f\"‚úÖ Dataset loaded: {len(df)} rows, {len(df.columns)} columns\")\n",
    "    print(f\"\\nColumns: {', '.join(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e3b73b",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Basic Dataset Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727e757c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic info\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af760c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types and missing values\n",
    "print(\"Data Types and Missing Values:\")\n",
    "info_df = pd.DataFrame({\n",
    "    'Type': df.dtypes,\n",
    "    'Missing': df.isnull().sum(),\n",
    "    'Missing %': (df.isnull().sum() / len(df) * 100).round(2)\n",
    "})\n",
    "info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe764e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary of numeric features\n",
    "print(\"\\nStatistical Summary:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eab2846",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Target Variable Analysis - Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce89446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Count plot\n",
    "class_counts = df['future_need_level'].value_counts()\n",
    "sns.barplot(x=class_counts.index, y=class_counts.values, ax=axes[0], palette='viridis')\n",
    "axes[0].set_title('Distribution of Future Need Level', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Need Level')\n",
    "axes[0].set_ylabel('Count')\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, v in enumerate(class_counts.values):\n",
    "    axes[0].text(i, v + 0.5, str(v), ha='center', fontweight='bold')\n",
    "\n",
    "# Pie chart\n",
    "colors = ['#ff6b6b', '#ffd93d', '#6bcf7f']\n",
    "axes[1].pie(class_counts.values, labels=class_counts.index, autopct='%1.1f%%', \n",
    "            colors=colors, startangle=90)\n",
    "axes[1].set_title('Percentage Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print statistics\n",
    "print(\"\\nClass Distribution:\")\n",
    "for level, count in class_counts.items():\n",
    "    percentage = (count / len(df)) * 100\n",
    "    print(f\"  {level}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "# Check for imbalance\n",
    "max_class = class_counts.max()\n",
    "min_class = class_counts.min()\n",
    "imbalance_ratio = max_class / min_class\n",
    "print(f\"\\n‚öñÔ∏è Imbalance Ratio: {imbalance_ratio:.2f}\")\n",
    "if imbalance_ratio > 3:\n",
    "    print(\"‚ö†Ô∏è WARNING: Significant class imbalance detected!\")\n",
    "else:\n",
    "    print(\"‚úÖ Classes are reasonably balanced.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827ec4da",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Categorical Features Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14411660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze job roles\n",
    "print(\"Top 10 Job Roles by Frequency:\")\n",
    "job_role_counts = df['job_role_name'].value_counts().head(10)\n",
    "print(job_role_counts)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=job_role_counts.values, y=job_role_counts.index, palette='coolwarm')\n",
    "plt.title('Top 10 Job Roles', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Job Role')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9ce520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze skills\n",
    "print(\"\\nTop 10 Skills by Frequency:\")\n",
    "skill_counts = df['skill_name'].value_counts().head(10)\n",
    "print(skill_counts)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=skill_counts.values, y=skill_counts.index, palette='plasma')\n",
    "plt.title('Top 10 Skills', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Skill')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a18b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skill categories and job departments\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Skill categories\n",
    "if 'skill_category' in df.columns:\n",
    "    skill_cat_counts = df['skill_category'].value_counts()\n",
    "    sns.barplot(x=skill_cat_counts.values, y=skill_cat_counts.index, ax=axes[0], palette='viridis')\n",
    "    axes[0].set_title('Skill Categories', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_xlabel('Count')\n",
    "\n",
    "# Job departments\n",
    "if 'job_department' in df.columns:\n",
    "    dept_counts = df['job_department'].value_counts()\n",
    "    sns.barplot(x=dept_counts.values, y=dept_counts.index, ax=axes[1], palette='magma')\n",
    "    axes[1].set_title('Job Departments', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_xlabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3029eb0e",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Numeric Features Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f68b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numeric columns\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(f\"Numeric features: {', '.join(numeric_cols)}\")\n",
    "\n",
    "# Distribution of numeric features\n",
    "n_cols = len(numeric_cols)\n",
    "n_rows = (n_cols + 2) // 3\n",
    "fig, axes = plt.subplots(n_rows, 3, figsize=(15, n_rows * 4))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(numeric_cols):\n",
    "    axes[i].hist(df[col], bins=30, edgecolor='black', color='steelblue', alpha=0.7)\n",
    "    axes[i].set_title(f'Distribution of {col}', fontweight='bold')\n",
    "    axes[i].set_xlabel(col)\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "    \n",
    "    # Add statistics\n",
    "    mean_val = df[col].mean()\n",
    "    median_val = df[col].median()\n",
    "    axes[i].axvline(mean_val, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_val:.2f}')\n",
    "    axes[i].axvline(median_val, color='green', linestyle='--', linewidth=2, label=f'Median: {median_val:.2f}')\n",
    "    axes[i].legend()\n",
    "\n",
    "# Hide unused subplots\n",
    "for i in range(n_cols, len(axes)):\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6666c715",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af39d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots for outlier detection\n",
    "fig, axes = plt.subplots(n_rows, 3, figsize=(15, n_rows * 4))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(numeric_cols):\n",
    "    sns.boxplot(y=df[col], ax=axes[i], color='lightblue')\n",
    "    axes[i].set_title(f'Box Plot - {col}', fontweight='bold')\n",
    "    axes[i].set_ylabel(col)\n",
    "    \n",
    "    # Calculate and display outliers\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    outliers = df[(df[col] < Q1 - 1.5 * IQR) | (df[col] > Q3 + 1.5 * IQR)]\n",
    "    axes[i].text(0.05, 0.95, f'Outliers: {len(outliers)}', \n",
    "                transform=axes[i].transAxes, verticalalignment='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# Hide unused subplots\n",
    "for i in range(n_cols, len(axes)):\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5c4eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed outlier statistics\n",
    "print(\"\\nOutlier Statistics (using IQR method):\")\n",
    "for col in numeric_cols:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "    outlier_percentage = (len(outliers) / len(df)) * 100\n",
    "    print(f\"  {col}: {len(outliers)} outliers ({outlier_percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a11924",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Feature Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ca368b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "correlation_matrix = df[numeric_cols].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Feature Correlation Matrix', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print strong correlations\n",
    "print(\"\\nStrong Correlations (|correlation| > 0.5):\")\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.5:\n",
    "            print(f\"  {correlation_matrix.columns[i]} ‚Üî {correlation_matrix.columns[j]}: {correlation_matrix.iloc[i, j]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02390d64",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Feature Distribution by Target Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b29cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots by target class\n",
    "n_cols_show = min(6, len(numeric_cols))  # Show first 6 numeric features\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(numeric_cols[:n_cols_show]):\n",
    "    sns.boxplot(x='future_need_level', y=col, data=df, ax=axes[i], \n",
    "                palette='Set2', order=['LOW', 'MEDIUM', 'HIGH'])\n",
    "    axes[i].set_title(f'{col} by Need Level', fontweight='bold')\n",
    "    axes[i].set_xlabel('Future Need Level')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba68111c",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afce1a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìã DATA QUALITY REPORT\\n\" + \"=\"*50)\n",
    "\n",
    "# 1. Missing values\n",
    "missing_count = df.isnull().sum().sum()\n",
    "print(f\"\\n1. Missing Values: {missing_count}\")\n",
    "if missing_count == 0:\n",
    "    print(\"   ‚úÖ No missing values detected\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è Missing values found:\")\n",
    "    print(df.isnull().sum()[df.isnull().sum() > 0])\n",
    "\n",
    "# 2. Duplicate rows\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"\\n2. Duplicate Rows: {duplicates}\")\n",
    "if duplicates == 0:\n",
    "    print(\"   ‚úÖ No duplicate rows\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è Duplicate rows found\")\n",
    "\n",
    "# 3. Target class distribution\n",
    "print(f\"\\n3. Target Class Balance:\")\n",
    "class_dist = df['future_need_level'].value_counts(normalize=True) * 100\n",
    "for level, pct in class_dist.items():\n",
    "    status = \"‚úÖ\" if 20 <= pct <= 50 else \"‚ö†Ô∏è\"\n",
    "    print(f\"   {status} {level}: {pct:.1f}%\")\n",
    "\n",
    "# 4. Feature value ranges\n",
    "print(f\"\\n4. Feature Value Ranges:\")\n",
    "for col in numeric_cols:\n",
    "    min_val = df[col].min()\n",
    "    max_val = df[col].max()\n",
    "    # Check if values are in expected range [0, 1] for most features\n",
    "    if col != 'avg_salary_k':\n",
    "        status = \"‚úÖ\" if 0 <= min_val and max_val <= 1 else \"‚ö†Ô∏è\"\n",
    "        print(f\"   {status} {col}: [{min_val:.3f}, {max_val:.3f}]\")\n",
    "    else:\n",
    "        status = \"‚úÖ\" if min_val > 0 and max_val < 200 else \"‚ö†Ô∏è\"\n",
    "        print(f\"   {status} {col}: [{min_val:.2f}, {max_val:.2f}] K‚Ç¨\")\n",
    "\n",
    "# 5. Unique combinations\n",
    "unique_combos = df[['job_role_name', 'skill_name']].drop_duplicates().shape[0]\n",
    "print(f\"\\n5. Unique (Job Role, Skill) Combinations: {unique_combos}\")\n",
    "print(f\"   Total rows: {len(df)}\")\n",
    "if unique_combos == len(df):\n",
    "    print(\"   ‚úÖ All combinations are unique\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è Some combinations appear multiple times\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7653d5",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de76be51",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüéØ DATASET SUMMARY AND RECOMMENDATIONS\\n\" + \"=\"*60)\n",
    "\n",
    "print(f\"\\nüìä Dataset Size:\")\n",
    "print(f\"   ‚Ä¢ Total samples: {len(df)}\")\n",
    "print(f\"   ‚Ä¢ Features: {len(df.columns)}\")\n",
    "print(f\"   ‚Ä¢ Unique job roles: {df['job_role_name'].nunique()}\")\n",
    "print(f\"   ‚Ä¢ Unique skills: {df['skill_name'].nunique()}\")\n",
    "\n",
    "print(f\"\\nüéØ Target Distribution:\")\n",
    "class_counts = df['future_need_level'].value_counts()\n",
    "for level in ['LOW', 'MEDIUM', 'HIGH']:\n",
    "    if level in class_counts:\n",
    "        count = class_counts[level]\n",
    "        pct = (count / len(df)) * 100\n",
    "        print(f\"   ‚Ä¢ {level}: {count} ({pct:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüí° Recommendations:\")\n",
    "\n",
    "# Check class imbalance\n",
    "imbalance_ratio = class_counts.max() / class_counts.min()\n",
    "if imbalance_ratio > 3:\n",
    "    print(\"   ‚ö†Ô∏è CLASS IMBALANCE DETECTED (ratio > 3)\")\n",
    "    print(\"      ‚Üí Consider using class_weight='balanced' in the model\")\n",
    "    print(\"      ‚Üí Consider SMOTE or other resampling techniques\")\n",
    "    print(\"      ‚Üí Use stratified train/test split\")\n",
    "else:\n",
    "    print(\"   ‚úÖ Classes are reasonably balanced\")\n",
    "\n",
    "# Check dataset size\n",
    "if len(df) < 100:\n",
    "    print(\"   ‚ö†Ô∏è SMALL DATASET (< 100 samples)\")\n",
    "    print(\"      ‚Üí Add more job roles and skills to increase diversity\")\n",
    "    print(\"      ‚Üí Consider using cross-validation instead of train/test split\")\n",
    "elif len(df) < 500:\n",
    "    print(\"   ‚ö†Ô∏è MODERATE DATASET SIZE (< 500 samples)\")\n",
    "    print(\"      ‚Üí Use cross-validation for better model evaluation\")\n",
    "    print(\"      ‚Üí Consider data augmentation if possible\")\n",
    "else:\n",
    "    print(\"   ‚úÖ Good dataset size for training\")\n",
    "\n",
    "# Check for outliers\n",
    "total_outliers = 0\n",
    "for col in numeric_cols:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    outliers = len(df[(df[col] < Q1 - 1.5 * IQR) | (df[col] > Q3 + 1.5 * IQR)])\n",
    "    total_outliers += outliers\n",
    "\n",
    "if total_outliers > len(df) * 0.05:  # More than 5% outliers\n",
    "    print(\"   ‚ö†Ô∏è MANY OUTLIERS DETECTED (> 5% of data)\")\n",
    "    print(\"      ‚Üí Review outliers to ensure they're realistic\")\n",
    "    print(\"      ‚Üí Consider robust scaling or outlier removal\")\n",
    "else:\n",
    "    print(\"   ‚úÖ Acceptable number of outliers\")\n",
    "\n",
    "print(\"\\n‚ú® Next Steps:\")\n",
    "print(\"   1. If satisfied with the data quality, proceed with model training\")\n",
    "print(\"   2. Update the training script to use new features\")\n",
    "print(\"   3. Run: python ../manage.py export_future_skills_dataset\")\n",
    "print(\"   4. Run: python train_future_skills_model.py\")\n",
    "print(\"   5. Evaluate model performance and iterate\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
