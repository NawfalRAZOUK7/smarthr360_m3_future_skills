# ðŸ” Explainability (SHAP/LIME) - README

## Introduction

Ce dossier contient l'implÃ©mentation complÃ¨te du **LT-1 â€” Explainability** pour le Module 3 Future Skills. L'objectif est d'expliquer **pourquoi** le modÃ¨le ML recommande une compÃ©tence comme HIGH, MEDIUM ou LOW en utilisant SHAP (SHapley Additive exPlanations) et LIME.

---

## ðŸ“‹ Table des matiÃ¨res

1. [Fichiers crÃ©Ã©s](#-fichiers-crÃ©Ã©s)
2. [Installation rapide](#-installation-rapide)
3. [DÃ©marrage rapide](#-dÃ©marrage-rapide)
4. [Documentation](#-documentation)
5. [Architecture](#-architecture)
6. [Prochaines Ã©tapes](#-prochaines-Ã©tapes)

---

## ðŸ“ Fichiers crÃ©Ã©s

### Nouveaux fichiers

| Fichier                                                  | Description                                 |
| -------------------------------------------------------- | ------------------------------------------- |
| `ml/explainability_analysis.ipynb`                       | Notebook interactif avec analyses SHAP/LIME |
| `future_skills/services/explanation_engine.py`           | Moteur de gÃ©nÃ©ration d'explications         |
| `future_skills/migrations/0005_add_explanation_field.py` | Migration DB pour champ `explanation`       |
| `docs/LT1_EXPLAINABILITY_GUIDE.md`                       | Guide complet d'explicabilitÃ©               |
| `docs/LT1_COMPLETION_SUMMARY.md`                         | RÃ©sumÃ© de l'implÃ©mentation                  |
| `docs/LT1_QUICK_COMMANDS.md`                             | Commandes rapides                           |
| `ml/README_EXPLAINABILITY.md`                            | Ce fichier                                  |

### Fichiers modifiÃ©s

| Fichier                                       | Modifications                            |
| --------------------------------------------- | ---------------------------------------- |
| `requirements_ml.txt`                         | Ajout de shap, lime, matplotlib, seaborn |
| `future_skills/models.py`                     | Ajout du champ `explanation` (JSONField) |
| `future_skills/services/prediction_engine.py` | IntÃ©gration de l'ExplanationEngine       |
| `QUICK_COMMANDS.md`                           | Section Explainability                   |

---

## ðŸš€ Installation rapide

```bash
# 1. Installer les dÃ©pendances
pip install -r requirements_ml.txt

# 2. Appliquer la migration
python manage.py migrate future_skills

# 3. VÃ©rifier que SHAP est disponible
python -c "import shap; print(f'âœ… SHAP version: {shap.__version__}')"
```

---

## âš¡ DÃ©marrage rapide

### Option 1 : Analyse interactive (Notebook)

```bash
# Lancer le notebook Jupyter
jupyter notebook ml/explainability_analysis.ipynb

# Ou avec JupyterLab
jupyter lab ml/explainability_analysis.ipynb
```

Le notebook contient :

- âœ… Analyses SHAP sur exemples HIGH/MEDIUM
- âœ… Visualisations (force plots, waterfall, summary)
- âœ… Analyses LIME alternatives
- âœ… GÃ©nÃ©ration d'explications simplifiÃ©es

### Option 2 : GÃ©nÃ©ration programmatique

```python
from future_skills.services.explanation_engine import ExplanationEngine
from future_skills.ml_model import FutureSkillsModel

# Charger le modÃ¨le
model = FutureSkillsModel.instance()
engine = ExplanationEngine(model)

# GÃ©nÃ©rer une explication
explanation = engine.generate_explanation(
    job_role_name="Data Engineer",
    skill_name="Python",
    trend_score=0.85,
    internal_usage=0.3,
    training_requests=12,
    scarcity_index=0.7
)

print(explanation["text"])
# Output: "Score Ã©levÃ© car : tendance marchÃ© forte + raretÃ© interne importante"
```

### Option 3 : Recalcul complet avec explications

```python
from future_skills.services.prediction_engine import recalculate_predictions

# Recalculer toutes les prÃ©dictions avec gÃ©nÃ©ration d'explications
total = recalculate_predictions(
    horizon_years=5,
    generate_explanations=True  # Active SHAP
)

print(f"âœ… {total} prÃ©dictions gÃ©nÃ©rÃ©es avec explications")
```

---

## ðŸ“– Documentation

### Documents clÃ©s

1. **[LT1_EXPLAINABILITY_GUIDE.md](docs/LT1_EXPLAINABILITY_GUIDE.md)** - Guide complet

   - Architecture dÃ©taillÃ©e
   - Format d'explication JSON
   - Exemples d'utilisation
   - IntÃ©gration API/UI
   - Tests et troubleshooting

2. **[LT1_COMPLETION_SUMMARY.md](docs/LT1_COMPLETION_SUMMARY.md)** - RÃ©sumÃ© d'implÃ©mentation

   - Objectifs atteints âœ…
   - Composants crÃ©Ã©s
   - Exemples concrets
   - Prochaines Ã©tapes

3. **[LT1_QUICK_COMMANDS.md](docs/LT1_QUICK_COMMANDS.md)** - Commandes rapides
   - Installation
   - Tests
   - Debugging
   - Maintenance

### Ordre de lecture recommandÃ©

1. Ce README (vue d'ensemble)
2. `LT1_COMPLETION_SUMMARY.md` (qu'est-ce qui a Ã©tÃ© fait ?)
3. `LT1_EXPLAINABILITY_GUIDE.md` (comment Ã§a marche ?)
4. `LT1_QUICK_COMMANDS.md` (comment l'utiliser ?)

---

## ðŸ—ï¸ Architecture

### Flux de donnÃ©es

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ML Model          â”‚
â”‚  (RandomForest)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ExplanationEngine   â”‚
â”‚  - SHAP Calculator  â”‚
â”‚  - Feature Mapper   â”‚
â”‚  - Text Generator   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Explanation       â”‚
â”‚  {                  â”‚
â”‚    text: "...",     â”‚
â”‚    top_factors: [], â”‚
â”‚    confidence: 87.5 â”‚
â”‚  }                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FutureSkillPredictionâ”‚
â”‚  .explanation       â”‚
â”‚  (JSONField)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Composants principaux

1. **ExplanationEngine** (`future_skills/services/explanation_engine.py`)

   - Calcule les SHAP values
   - Identifie les top features
   - GÃ©nÃ¨re du texte simple
   - Fallback gracieux si SHAP indisponible

2. **Notebook** (`ml/explainability_analysis.ipynb`)

   - Analyse interactive
   - Visualisations riches
   - ExpÃ©rimentation

3. **ModÃ¨le DB** (`future_skills/models.py`)

   - Champ `explanation` (JSONField)
   - Stockage persistant
   - RequÃªtes optimisÃ©es

4. **IntÃ©gration** (`future_skills/services/prediction_engine.py`)
   - ParamÃ¨tre `generate_explanations`
   - GÃ©nÃ©ration optionnelle
   - Logging dÃ©taillÃ©

---

## ðŸŽ¯ Format d'explication

### Structure JSON

```json
{
  "text": "Score Ã©levÃ© car : tendance marchÃ© forte + raretÃ© interne importante",
  "top_factors": [
    {
      "feature": "trend_score",
      "feature_readable": "tendance marchÃ©",
      "impact": "positive",
      "strength": "forte",
      "shap_value": 0.3245
    },
    {
      "feature": "scarcity_index",
      "feature_readable": "raretÃ© interne",
      "impact": "positive",
      "strength": "importante",
      "shap_value": 0.2156
    }
  ],
  "prediction_level": "HIGH",
  "confidence": 87.5
}
```

### Mapping des features

| Feature technique   | Terme mÃ©tier          |
| ------------------- | --------------------- |
| `trend_score`       | tendance marchÃ©       |
| `scarcity_index`    | raretÃ© interne        |
| `internal_usage`    | usage interne actuel  |
| `training_requests` | demandes de formation |

---

## ðŸ§ª Tests

### VÃ©rifier l'installation

```bash
# Test 1 : SHAP disponible
python -c "import shap; print('âœ… SHAP OK')"

# Test 2 : ExplanationEngine chargeable
python manage.py shell -c "from future_skills.services.explanation_engine import ExplanationEngine; print('âœ… Engine OK')"

# Test 3 : Migration appliquÃ©e
python manage.py showmigrations future_skills | grep "0005_add_explanation_field"
```

### GÃ©nÃ©rer un exemple

```python
python manage.py shell
>>> from future_skills.services.explanation_engine import ExplanationEngine
>>> from future_skills.ml_model import FutureSkillsModel
>>>
>>> model = FutureSkillsModel.instance()
>>> if not model.is_available():
>>>     print("âš ï¸  ModÃ¨le ML non disponible - entraÃ®ner d'abord")
>>>     exit()
>>>
>>> engine = ExplanationEngine(model)
>>> if not engine.is_available():
>>>     print("âš ï¸  SHAP non disponible - vÃ©rifier installation")
>>>     exit()
>>>
>>> explanation = engine.generate_explanation(
...     job_role_name="Data Engineer",
...     skill_name="Python",
...     trend_score=0.85,
...     internal_usage=0.3,
...     training_requests=12,
...     scarcity_index=0.7
... )
>>>
>>> print("\nâœ… EXPLICATION GÃ‰NÃ‰RÃ‰E:")
>>> print(f"   {explanation['text']}")
>>> print(f"\n   Niveau: {explanation['prediction_level']}")
>>> print(f"   Confiance: {explanation['confidence']}%")
>>> print("\n   Top factors:")
>>> for factor in explanation['top_factors']:
...     print(f"     â€¢ {factor['feature_readable']}: {factor['strength']}")
```

---

## ðŸš¦ Prochaines Ã©tapes

### Phase 1 : Validation (court terme)

- [x] âœ… Notebook d'analyse crÃ©Ã©
- [x] âœ… ExplanationEngine implÃ©mentÃ©
- [x] âœ… Champ DB ajoutÃ©
- [x] âœ… Documentation complÃ¨te
- [ ] Tester sur dataset rÃ©el
- [ ] Ajuster les seuils de force

### Phase 2 : API Backend (moyen terme)

- [ ] CrÃ©er endpoint `/api/predictions/{id}/explain/`
- [ ] Ajouter paramÃ¨tre `?include_explanation=true`
- [ ] Serializer avec `explanation_text`
- [ ] Tests API

### Phase 3 : UI Frontend (long terme)

- [ ] Widget "Pourquoi cette compÃ©tence ?"
- [ ] Cartes d'explication
- [ ] Visualisations interactives
- [ ] Tests utilisateurs RH

### Phase 4 : Optimisation (futur)

- [ ] Cache des explications
- [ ] Batch processing
- [ ] A/B testing formats
- [ ] Personnalisation par rÃ´le

---

## ðŸ’¡ Conseils d'utilisation

### Performance

âš ï¸ **SHAP est coÃ»teux** : GÃ©nÃ©ration d'explications = ~1-2 secondes par prÃ©diction

âœ… **Solutions** :

1. GÃ©nÃ©rer en batch (lors du recalcul nocturne)
2. Stocker dans DB (champ `explanation`)
3. GÃ©nÃ©rer Ã  la demande uniquement pour HIGH

### Fallback

Si SHAP indisponible, l'engine utilise des rÃ¨gles simples :

- Analyse des seuils (trend_score > 0.7, etc.)
- Explications basiques mais comprÃ©hensibles
- DÃ©gradation gracieuse

### Debug

```bash
# VÃ©rifier les logs
tail -f logs/predictions.log | grep -i "explanation"

# Compter les explications gÃ©nÃ©rÃ©es
python manage.py shell -c "from future_skills.models import FutureSkillPrediction; print(FutureSkillPrediction.objects.filter(explanation__isnull=False).count())"
```

---

## ðŸŽ“ Ressources

- **SHAP** : https://github.com/slundberg/shap
- **LIME** : https://github.com/marcotcr/lime
- **Paper SHAP** : [Lundberg & Lee (2017)](https://arxiv.org/abs/1705.07874)
- **Paper LIME** : [Ribeiro et al. (2016)](https://arxiv.org/abs/1602.04938)

---

## ðŸ“ž Support

Pour toute question :

1. Consulter `docs/LT1_EXPLAINABILITY_GUIDE.md`
2. Voir `docs/LT1_QUICK_COMMANDS.md`
3. VÃ©rifier les logs : `logs/predictions.log`
4. Contacter l'Ã©quipe ML SmartHR360

---

**Status** : âœ… LT-1 Complet  
**Version** : 1.0  
**Date** : Novembre 2025
