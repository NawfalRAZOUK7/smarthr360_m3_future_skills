# ml/train_future_skills_model.py

"""ML model training script for Module 3 - Future Skills.

This script:
- Loads the dataset CSV generated by:
    python manage.py export_future_skills_dataset
- Builds a scikit-learn pipeline:
    - OneHotEncoder for job_role_name and skill_name
    - StandardScaler for numeric features
    - RandomForestClassifier as the classification model
- Saves the complete pipeline (preprocessing + model) to:
    artifacts/models/future_skills_model_vX.pkl (with versioning)
- Generates a JSON metadata file for MLOps traceability

This script is intentionally Django-independent (no settings required).
"""

import argparse
import json
import logging
from datetime import datetime
from pathlib import Path

import joblib
import numpy as np
import pandas as pd
from sklearn.compose import ColumnTransformer
from sklearn.ensemble import RandomForestClassifier
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_recall_fscore_support
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler

PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent
ARTIFACTS_DIR = PROJECT_ROOT / "artifacts"
DEFAULT_DATASET_DIR = ARTIFACTS_DIR / "datasets"
DEFAULT_MODEL_DIR = ARTIFACTS_DIR / "models"
JOBLIB_CACHE_DIR = ARTIFACTS_DIR / "cache" / "joblib"

for directory in (
    ARTIFACTS_DIR,
    DEFAULT_DATASET_DIR,
    DEFAULT_MODEL_DIR,
    JOBLIB_CACHE_DIR,
):
    directory.mkdir(parents=True, exist_ok=True)

# Configure logging
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s")


ALLOWED_LEVELS = {"LOW", "MEDIUM", "HIGH"}


def load_dataset(csv_path: Path) -> pd.DataFrame:
    """Load and validate the future skills dataset from CSV.

    Args:
        csv_path: Path to the CSV file containing the dataset

    Returns:
        Filtered pandas DataFrame with validated future skills data

    Raises:
        FileNotFoundError: If the CSV file doesn't exist
        ValueError: If required target column is missing
    """
    if not csv_path.exists():
        raise FileNotFoundError(f"Dataset CSV not found: {csv_path}")

    df = pd.read_csv(csv_path)

    # Accept either 'future_need_level' or 'level' as the target column
    target_col = None
    if "future_need_level" in df.columns:
        target_col = "future_need_level"
    elif "level" in df.columns:
        target_col = "level"
    else:
        raise ValueError("Target column 'future_need_level' or 'level' is missing from dataset.")

    # Filter for allowed levels (LOW / MEDIUM / HIGH)
    before = len(df)
    df = df[df[target_col].isin(ALLOWED_LEVELS)].copy()
    after = len(df)

    if after == 0:
        raise ValueError(f"No valid rows with future_need_level in {ALLOWED_LEVELS}.")

    if after < before:
        logger.warning(f"{before - after} row(s) filtered out where future_need_level " f"was not in {ALLOWED_LEVELS}.")

    # Attach target_col as attribute for downstream use
    df._target_col = target_col
    return df


def build_pipeline(
    categorical_features,
    numeric_features,
    n_estimators: int = 200,
    random_state: int = 42,
) -> Pipeline:
    """Build a complete ML pipeline.

    - ColumnTransformer (OneHot + StandardScaler)
    - RandomForestClassifier
    """
    categorical_transformer = OneHotEncoder(handle_unknown="ignore")

    # Impute missing numeric values with mean, then scale
    numeric_transformer = Pipeline(
        [
            ("imputer", SimpleImputer(strategy="mean")),
            ("scaler", StandardScaler()),
        ]
    )

    preprocessor = ColumnTransformer(
        transformers=[
            ("cat", categorical_transformer, categorical_features),
            ("num", numeric_transformer, numeric_features),
        ]
    )

    clf = RandomForestClassifier(
        n_estimators=n_estimators,
        random_state=random_state,
        class_weight="balanced",
        n_jobs=-1,
    )

    pipeline = Pipeline(
        steps=[
            ("preprocess", preprocessor),
            ("clf", clf),
        ],
        memory=str(JOBLIB_CACHE_DIR),  # Cache transformers for better performance  # noqa: S106 - Not a password
    )

    return pipeline


def _prepare_features(df: pd.DataFrame, feature_cols: list, target_col: str) -> tuple:
    """Prepare features and target, handling missing columns."""
    available_features = [c for c in feature_cols if c in df.columns]
    missing_cols = [c for c in feature_cols if c not in df.columns]

    if missing_cols:
        print(f"[WARN] Colonnes manquantes (ignor√©es) : {missing_cols}")

    if not available_features:
        raise ValueError("Aucune feature disponible dans le dataset!")

    X = df[available_features].copy()
    y = df[target_col].copy()

    return X, y, available_features, missing_cols


def _identify_feature_types(df: pd.DataFrame, available_features: list) -> tuple:
    """Identify categorical and numeric features dynamically."""
    categorical_features = []
    numeric_features = []

    for col in available_features:
        if df[col].dtype == "object" or df[col].dtype.name == "category":
            categorical_features.append(col)
        else:
            numeric_features.append(col)

    return categorical_features, numeric_features


def _check_class_imbalance(y: pd.Series) -> tuple:
    """Check for class imbalance and print warnings. Returns (imbalance_ratio, class_counts)."""
    class_counts = y.value_counts()
    imbalance_ratio = class_counts.max() / class_counts.min()
    print(f"[INFO] Ratio de d√©s√©quilibre : {imbalance_ratio:.2f}")
    if imbalance_ratio > 3:
        print("[WARN] D√©s√©quilibre des classes d√©tect√©. Utilisation de class_weight='balanced'")
    return imbalance_ratio, class_counts


def _compute_per_class_metrics(cm: np.ndarray) -> dict:
    """Calculate per-class metrics from confusion matrix."""
    per_class_metrics = {}
    print("\nPr√©cision par classe :")
    for i, level in enumerate(["LOW", "MEDIUM", "HIGH"]):
        if cm.sum(axis=1)[i] > 0:
            class_accuracy = cm[i, i] / cm.sum(axis=1)[i]
            support_count = int(cm.sum(axis=1)[i])
            per_class_metrics[level] = {
                "accuracy": round(float(class_accuracy), 4),
                "support": support_count,
            }
            print(f"  {level}: {class_accuracy:.2%} (support: {support_count})")
        else:
            per_class_metrics[level] = {"accuracy": 0.0, "support": 0}
            print(f"  {level}: N/A (no samples)")
    return per_class_metrics


def train_model(
    csv_path: Path,
    output_model_path: Path,
    model_version: str = "v1",
    test_size: float = 0.2,
    random_state: int = 42,
    n_estimators: int = 200,
):
    """Train the Future Skills ML model and save with metadata.

    Args:
        csv_path: Path to the input dataset CSV
        output_model_path: Path where the model will be saved
        model_version: Version identifier (e.g., 'v1', 'v2', 'v2.1')
        test_size: Proportion of test set
        random_state: Random seed for reproducibility
        n_estimators: Number of trees in RandomForest
    """
    training_start_time = datetime.now()

    print(f"[INFO] Chargement du dataset : {csv_path}")
    df = load_dataset(csv_path)
    # Use the detected target column
    target_col = getattr(df, "_target_col", "future_need_level")

    # D√©finir les features et la cible (UPDATED with new features)
    feature_cols = [
        "job_role_name",
        "skill_name",
        "skill_category",
        "job_department",
        "trend_score",
        "internal_usage",
        "training_requests",
        "scarcity_index",
        "hiring_difficulty",
        "avg_salary_k",
        "economic_indicator",
    ]

    # Prepare features and target
    X, y, available_features, missing_cols = _prepare_features(df, feature_cols, target_col)

    # Identify feature types
    categorical_features, numeric_features = _identify_feature_types(df, available_features)

    print(f"[INFO] Features cat√©gorielles : {categorical_features}")
    print(f"[INFO] Features num√©riques : {numeric_features}")
    print(f"[INFO] Nombre total d'exemples : {len(df)}")
    print("[INFO] R√©partition des classes :")
    print(y.value_counts())

    # Check for class imbalance
    imbalance_ratio, class_counts = _check_class_imbalance(y)

    X_train, X_test, y_train, y_test = train_test_split(
        X,
        y,
        test_size=test_size,
        random_state=random_state,
        stratify=y,
    )

    print(f"[INFO] Taille train : {len(X_train)}, test : {len(X_test)}")

    # Build and train pipeline
    pipeline = build_pipeline(
        categorical_features=categorical_features,
        numeric_features=numeric_features,
        n_estimators=n_estimators,
        random_state=random_state,
    )

    print(f"[INFO] Entra√Ænement du mod√®le RandomForestClassifier (n_estimators={n_estimators})...")
    pipeline.fit(X_train, y_train)

    training_end_time = datetime.now()
    training_duration = (training_end_time - training_start_time).total_seconds()

    # Evaluation
    print("[INFO] √âvaluation sur le set de test :")
    y_pred = pipeline.predict(X_test)

    # Compute metrics
    accuracy = accuracy_score(y_test, y_pred)
    precision, recall, f1, _ = precision_recall_fscore_support(
        y_test, y_pred, labels=["LOW", "MEDIUM", "HIGH"], average="weighted"
    )

    print("\nClassification report :")
    print(classification_report(y_test, y_pred, digits=4))

    print("\nMatrice de confusion :")
    cm = confusion_matrix(y_test, y_pred, labels=["LOW", "MEDIUM", "HIGH"])
    print(cm)

    # Calculate per-class metrics using helper function
    per_class_metrics = _compute_per_class_metrics(cm)

    # Feature importance
    clf = pipeline.named_steps["clf"]  # noqa: PD011 - sklearn pipeline access pattern
    print(f"[INFO] Classes apprises par le mod√®le : {clf.classes_}")

    feature_importance_dict = {}
    if hasattr(clf, "feature_importances_"):
        print("\n[INFO] Importance des features :")
        preprocessor = pipeline.named_steps["preprocess"]  # noqa: PD011 - sklearn pipeline access pattern

        # Get feature names after preprocessing
        cat_features = []
        if categorical_features:
            cat_transformer = preprocessor.named_transformers_["cat"]  # noqa: PD011 - sklearn pipeline access pattern
            if hasattr(cat_transformer, "get_feature_names_out"):
                cat_features = cat_transformer.get_feature_names_out(categorical_features).tolist()

        all_features = cat_features + numeric_features

        if len(all_features) == len(clf.feature_importances_):
            feature_importance = sorted(
                zip(all_features, clf.feature_importances_),
                key=lambda x: x[1],
                reverse=True,
            )
            for feat, importance in feature_importance[:10]:  # Top 10
                print(f"  {feat}: {importance:.4f}")
                feature_importance_dict[feat] = float(importance)

    # Sauvegarde du pipeline complet
    output_model_path.parent.mkdir(parents=True, exist_ok=True)
    joblib.dump(pipeline, output_model_path)
    print(f"\n[SUCCESS] Mod√®le sauvegard√© dans : {output_model_path}")

    # Generate and save metadata
    metadata = {
        "model_version": model_version,
        "training_date": training_start_time.isoformat(),
        "training_duration_seconds": round(training_duration, 2),
        "dataset": {
            "csv_path": str(csv_path),
            "total_samples": len(df),
            "train_samples": len(X_train),
            "test_samples": len(X_test),
            "features_used": available_features,
            "features_missing": missing_cols,
            "categorical_features": categorical_features,
            "numeric_features": numeric_features,
            "class_distribution": {k: int(v) for k, v in class_counts.items()},
            "imbalance_ratio": round(float(imbalance_ratio), 2),
        },
        "hyperparameters": {
            "n_estimators": n_estimators,
            "random_state": random_state,
            "test_size": test_size,
            "class_weight": "balanced",
        },
        "metrics": {
            "accuracy": round(float(accuracy), 4),
            "precision_weighted": round(float(precision), 4),
            "recall_weighted": round(float(recall), 4),
            "f1_weighted": round(float(f1), 4),
            "per_class": per_class_metrics,
        },
        "feature_importance_top10": dict(list(feature_importance_dict.items())[:10]),
        "model_classes": clf.classes_.tolist(),
    }

    # Save metadata JSON
    metadata_path = output_model_path.with_suffix(".json")
    with open(metadata_path, "w", encoding="utf-8") as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)

    print(f"[SUCCESS] M√©tadonn√©es sauvegard√©es dans : {metadata_path}")
    print(f"[SUCCESS] Le mod√®le utilise {len(available_features)} features")

    return metadata


def main():
    """Main entry point for training the Future Skills ML model.

    Parses command line arguments and orchestrates the model training process.
    Supports configuration of dataset path, output location, model version,
    and training hyperparameters.
    """
    default_csv = DEFAULT_DATASET_DIR / "future_skills_dataset.csv"
    default_model = DEFAULT_MODEL_DIR / "future_skills_model.pkl"

    parser = argparse.ArgumentParser(description="Entra√Æne le mod√®le ML pour le Module 3 - Future Skills.")
    parser.add_argument(
        "--csv",
        type=str,
        default=str(default_csv),
        help=f"Chemin vers le dataset CSV (par d√©faut: {default_csv})",
    )
    parser.add_argument(
        "--output",
        type=str,
        default=str(default_model),
        help=f"Chemin de sortie du mod√®le .pkl (par d√©faut: {default_model})",
    )
    parser.add_argument(
        "--version",
        type=str,
        default="v1",
        help="Version du mod√®le (ex: v1, v2, v2.1). Utilis√© pour la tra√ßabilit√©.",
    )
    parser.add_argument(
        "--test-size",
        type=float,
        default=0.2,
        help="Proportion du set de test (par d√©faut: 0.2).",
    )
    parser.add_argument(
        "--random-state",
        type=int,
        default=42,
        help="Seed al√©atoire (par d√©faut: 42).",
    )
    parser.add_argument(
        "--n-estimators",
        type=int,
        default=200,
        help="Nombre d'arbres dans RandomForest (par d√©faut: 200).",
    )

    args = parser.parse_args()

    csv_path = Path(args.csv)
    output_model_path = Path(args.output)

    # If version is provided and output doesn't include version, add it
    if args.version and args.version not in output_model_path.stem:
        output_model_path = output_model_path.parent / f"{output_model_path.stem}_{args.version}.pkl"
        print(f"[INFO] Nom du mod√®le ajust√© avec version : {output_model_path}")

    metadata = train_model(
        csv_path=csv_path,
        output_model_path=output_model_path,
        model_version=args.version,
        test_size=args.test_size,
        random_state=args.random_state,
        n_estimators=args.n_estimators,
    )

    print("\n" + "=" * 60)
    print("üìä R√âSUM√â DE L'ENTRA√éNEMENT")
    print("=" * 60)
    print(f"Version: {metadata['model_version']}")
    print(f"Date: {metadata['training_date']}")
    print(f"Dur√©e: {metadata['training_duration_seconds']}s")
    print(f"Pr√©cision: {metadata['metrics']['accuracy']:.2%}")
    print(f"F1-score: {metadata['metrics']['f1_weighted']:.4f}")
    print("=" * 60)


if __name__ == "__main__":
    main()
